# -*- coding: utf-8 -*-
"""ej5.12_sutton_and_barto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GMZZOafwps_xlla5UBeonQOar6RMsiyX

# Solución al ejercicio 5.12 del libro de Sutton & Barto utilizando Off-policy Monte Carlo con every-visit importance sampling.

Restricciones del ambiente: Si vamos a chocar con la frontera en el siguiente paso temporal, reiniciamos la posición del agente (lo volvemos a colocar en la línea de salida con velocidad $\overrightarrow v = 0$.
Además, no podemos frenar a mitad de camino. 
El programa recibe como input el perfil de la carrera, que se lo paso por strings de acuerdo a como lo hace [Patrick Coady](https://gist.github.com/pat-coady/26fafa10b4d14234bfde0bb58277786d)
"""

import numpy as np
import itertools
import random

class Track():
  def __init__(self, course):
    self.course = None
    self.load_environment(course)
    self.n_rows, self.n_cols = self.course.shape
    self.max_v = 4
    self.min_v = 0
    a_components = list(range(-1,2))
    self.actions = np.array(list(itertools.product(
                            a_components, a_components)
                            ))
    self.n_actions = len(self.actions)
  
  def take_action(self, action):
    """
    Take action, return state' and reward
        
    Args:
      action: 2-tuple of requested change in velocity in x- and
              y-direction. valid action is -1, 0, +1 in each axis.
                
    Returns:
      reward: integer
    """
    if self.episode_ended():
        return 0
    self._update_velocity(action)
    self._update_position()
    return -1
    
  def is_wall(self, pos):
    """ A function to check if a state's position coordinates
    belong to the wall
    """
    return True if self.course[pos[0], pos[1]] == -1 else False

  def is_start(self, pos):
    """ A function to check if a state's position coordinates
    belong to the start line
    """
    return True if self.course[pos[0], pos[1]] == 0 else False  
  
  def is_finish(self, pos):
    """Return True at episode terminal state"""
    return (self.course[pos[0], pos[1]] == 2) 

  def generate_episode(self, pi, epsilon):
    while not track.episode_ended():
      s = track.get_state()
      s_x, s_y = s[0][0], s[0][1]
      s_vx, s_vy = s[1][0], s[1][1]
      if np.random.rand() > epsilon:
          a_i = pi[s_x, s_y, s_vx, s_vy]
      else:
          a_i = random.randrange(9)
      a = track.actions[a_i]
      R_t = track.take_action(a)
      episode.append((s, a, R_t))
    return episode

  def episode_ended(self):
    """Return True at episode terminal state"""
    return (self.course[self.position[0], self.position[1]] == 2) 

  def restart(self):
    start_p = np.transpose(np.where(self.course == 0))
    indices = start_p[np.random.choice(start_p.shape[0], 1, replace=False)]
    self.position = np.ndarray.flatten(indices)
    self.velocity = np.array([0, 0], dtype=np.int16)

  def get_state(self):
    return self.position.copy(), self.velocity.copy()

  def _update_velocity(self, action):  
    self.velocity += action
    self.velocity = np.minimum(self.velocity, self.max_v)
    self.velocity = np.maximum(self.velocity, self.min_v)
  
  def _update_position(self):
      for tstep in range(0, self.max_v + 1):
          t = tstep / self.max_v
          pos = self.position + np.round(self.velocity * t).astype(np.int16)
          if self.is_wall(pos): 
              self.restart()
              return
          if self.is_finish(pos):
              self.position = pos
              self.velocity = np.array([0, 0], dtype=np.int16)
              return
      self.position = pos


  def load_environment(self, string_racetrack):
    """A function to generate the environment in matrix form

    Parameters
    ----------
    string_track: list of strings 
      A string representing the environment
  
    Returns
    -------
    grid: np.ndarray
      An ndarray version of the environment
  
    """
    len_y = len(string_racetrack)
    len_x = len(string_racetrack[0])
    racetrack = np.zeros((len_x, len_y), dtype=np.int16)
    # Check if the input environment is correctly built:
    for y in range(len_y):
      for x in range(len_x):
        point = string_racetrack[y][x]
        if point == 'o':
          racetrack[x, y] = 1
        elif point == '-':
          racetrack[x, y] = 0
        elif point == '+':
          racetrack[x, y] = 2
        elif point == 'W':
          racetrack[x, y] = -1

    # Flip racetrack so (0,0) is in upper-left corner
    self.course = np.fliplr(racetrack)

## String environment from Fig. 5.6 (Sutton & Barto)

big_course =  [
              'WWWWWWWWWWWWWWWWWW',
              'WWWWooooooooooooo+',
              'WWWoooooooooooooo+',
              'WWWoooooooooooooo+',
              'WWooooooooooooooo+',
              'Woooooooooooooooo+',
              'Woooooooooooooooo+',
              'WooooooooooWWWWWWW',
              'WoooooooooWWWWWWWW',
              'WoooooooooWWWWWWWW',
              'WoooooooooWWWWWWWW',
              'WoooooooooWWWWWWWW',
              'WoooooooooWWWWWWWW',
              'WoooooooooWWWWWWWW',
              'WoooooooooWWWWWWWW',
              'WWooooooooWWWWWWWW',
              'WWooooooooWWWWWWWW',
              'WWooooooooWWWWWWWW',
              'WWooooooooWWWWWWWW',
              'WWooooooooWWWWWWWW',
              'WWooooooooWWWWWWWW',
              'WWooooooooWWWWWWWW',
              'WWooooooooWWWWWWWW',
              'WWWoooooooWWWWWWWW',
              'WWWoooooooWWWWWWWW',
              'WWWoooooooWWWWWWWW',
              'WWWoooooooWWWWWWWW',
              'WWWoooooooWWWWWWWW',
              'WWWoooooooWWWWWWWW',
              'WWWoooooooWWWWWWWW',
              'WWWWooooooWWWWWWWW',
              'WWWWooooooWWWWWWWW',
              'WWWW------WWWWWWWW']

tiny_course = ['WWWWWW',
               'Woooo+',
               'Woooo+',
               'WooWWW',
               'WooWWW',
               'WooWWW',
               'WooWWW',
               'W--WWW',]

track = Track(big_course)
n_rows, n_cols = track.n_rows, track.n_cols
Q = np.zeros((n_rows, n_cols, 5, 5, 3, 3)) - 40
C = np.zeros((n_rows, n_cols, 5, 5, 3, 3))
pi = np.zeros((n_rows, n_cols, 5, 5), dtype=np.int16) + 4
epsilon = 0.1
gamma = 0.9
n = 0
while n < 200000:
  track.restart()
  n += 1
  if (n+1) % 10000 == 0: print('Episode {}'.format(n+1))
  episode = track.generate_episode(pi, epsilon)
  # update Q and policy
  G = 0
  W = 1
  while len(episode) > 0:
      (s, a, R_t) = episode.pop()
      s_x, s_y = s[0][0], s[0][1]
      s_vx, s_vy = s[1][0], s[1][1]
      a_x, a_y = a
      s_t = (s_x, s_y, s_vx, s_vy, a_x, a_y)
      G = G * gamma + R_t
      C[s_t] += W
      Q[s_t] += W / C[s_t]  * (G - Q[s_t])
      # Find argmax
      q_max = -1e6
      for i in range(track.n_actions):
          act = track.actions[i]
          s_max = (s_x, s_y, s_vx, s_vy, act[0], act[1])
          if Q[s_max] > q_max:
            i_a_max = i
            a_max = track.actions[i_a_max]
            q_max = Q[s_max]
      pi[s_x, s_y, s_vx, s_vy] = i_a_max
      if not np.array_equal(a, a_max):
        break
      W *= 1/(1 - 8 / 9 * epsilon)

# Run learned policy on test case
import matplotlib.pyplot as plt

pos_map = np.zeros((track.n_rows, track.n_cols))
track.restart()
e = track.generate_episode(pi, epsilon=0.0)
while len(e) > 0:
    (s, a, R_t) = episode.pop()
    s_x, s_y = s[0][0], s[0][1]
    pos_map[s_x, s_y] += 1 

print('Sample trajectory on learned policy:')
pos_map = (pos_map > 0).astype(np.float32)
pos_map += track.course # overlay track course
plt.imshow(np.flipud(pos_map.T), cmap='hot', interpolation='nearest')
plt.show()